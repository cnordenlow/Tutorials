
# Python

## Basic

## Scrapy

*pip install Scrapy*

### Shell commands

*Run scrapy in shell: Write 'scrapy shell' in miniconda*

*Fetch page: Write 'Fetch ("https://www.riksbank.se/sv/penningpolitik/penningpolitiska-instrument/kop-av-foretagscertifikat/auction-results/2020/results-of-auctions-2020-09-23/")'*

*Use view(response)*


### Scrape table

```python
# Spider

import scrapy


class ToScrapeCSSSpider(scrapy.Spider):
    name = "rb_scrape2"
    start_urls = [

        'https://www.riksbank.se/sv/penningpolitik/penningpolitiska-instrument/kop-av-foretagscertifikat/auction-results/2020/results-of-auctions-2020-09-23/'
    ]

    def parse(self, response):


        for item in response.xpath('//*[@class="page-base__main__body"]//tr'):
            yield {

            'terms' : item.xpath('td[1]//text()').extract(),
            'auction' : item.xpath('td[2]//text()').extract()

            }

        next_page_url = response.css("li.next > a::attr(href)").extract_first()
        if next_page_url is not None:
            yield scrapy.Request(response.urljoin(next_page_url))


# Settings

#Export as CSV Feed
#FEED_FORMAT = "csv"
#FEED_URI = "rbbot.csv"

#Run crawler in miniconda with creating an csv (if its not in settings): scrapy crawl rb_scrape2 -o quotes.csv

```



### Scrape table json treasury scraper
```python
# Spider
# -*- coding: utf-8 -*-
#from scrapy import BaseSpider
import scrapy
#from TreasuryScraper.items import TreasuryItem
import json
class TreasurySpider(scrapy.Spider):
    name = 'treasury'
    start_urls = [
        'https://www.treasurydirect.gov/TA_WS/securities/jqsearch?format=json&filterscount=0&groupscount=0&pagenum=0&pagesize=1000&recordstartindex=0&recordendindex=1000',
    ]
    def parse(self, response):
        jsonresponse = json.loads(response.text)
        for item in jsonresponse['securityList']:
            yield {
                'cusip': item['cusip'],
                'securityType': item['securityType'],
                'securityTerm': item['securityTerm'],
                'offeringAmount': item['offeringAmount'],
                'tips': item['tips'],
                'type': item['type'],
                'pricePer100': item['pricePer100'],
                'floatingRate': item['floatingRate'],
                'reopening': item['reopening'],
                'auctionDate': item['auctionDate'],
                'maturityDate': item['maturityDate'],
                'term': item['term'],
                'competitiveAccepted': item['competitiveAccepted'],
                'allocationPercentage': item['allocationPercentage'],
                'averageMedianYield': item['averageMedianYield'],
                'bidToCoverRatio': item['bidToCoverRatio'],
                'competitiveAccepted': item['competitiveAccepted'],
                'highYield': item['highYield'],
                'lowYield': item['lowYield'],
                'somaAccepted': item['somaAccepted'],
                'somaHoldings': item['somaHoldings'],
                'primaryDealerAccepted': item['primaryDealerAccepted'],
                'directBidderAccepted': item['directBidderAccepted'],
                'directBidderTendered': item['directBidderTendered'],
                'indirectBidderAccepted': item['indirectBidderAccepted'],
                'indirectBidderTendered': item['indirectBidderTendered'],
                'interestPaymentFrequency': item['interestPaymentFrequency']

                }

#settings
# -*- coding: utf-8 -*-
#BOT_NAME = 'TreasuryScraper'
#SPIDER_MODULES = ['TreasuryScraper.spiders']
#NEWSPIDER_MODULE = 'TreasuryScraper.spiders'
#ROBOTSTXT_OBEY = False
#DOWNLOAD_DELAY = 60.0
#AUTOTHROTTLE_ENABLED = True
#HTTPCACHE_ENABLED = True 
#FEED_EXPORT_ENCODING = 'utf-8'

#scrapy crawl treasury -o output.csv

```

### Scrape table json auction results Riksbank
```python

import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from RiksbankAuctionScraper.items import GovernmentBond

def get_table_attr(response, x): # to handle strange tables
  # using normalize-space() to avoid "\n" as matching xpath
  xpaths = [
    './/td[contains(text(),"{0}")]/following-sibling::td/text()[normalize-space()]', #if td contains, use following sibling
    './/td[contains(text(),"{0}")]/following-sibling::td/p/text()[normalize-space()]',
    './/th/span[contains(text(),"{0}")]/../following-sibling::td/span/text()[normalize-space()]' #if th/span contains text, use following sibbling
  ]
  xpath_str = '|'.join(xpaths).format(x)
  return response.xpath(xpath_str).get()

class RiksbankSpider(CrawlSpider):
    name = "RiksbankAuctionScraper_v5"
    allowed_domains = ['riksbank.se']
    start_urls = [
      'https://www.riksbank.se/sv/penningpolitik/penningpolitiska-instrument/kop-av-statsobligationer/results-of-auctions'
    ]
    rules = (
      Rule(LinkExtractor(
        allow=('\/kop-av-statsobligationer\/results-of-auctions\/2020\/results-of-auctions-\d{4}-\d{2}-\d{2}\/$')
      ), callback='parse_government_bond'),
    )
    
    def parse_government_bond(self, response):
        for selector in response.xpath("//table"):
          item = GovernmentBond()
          item['auction_type'] = 'statsobligationer' 
          item['auction_date'] = get_table_attr(selector, "Auction date") #text to look for
          item['loan_number'] = get_table_attr(selector, "Loan")
          yield item
          
 #Items    
# -*- coding: utf-8 -*-
from scrapy import Item, Field

class GovernmentBonds(Item):
  auction_type = Field()
  auction_date = Field()
  loan_number= Field()         
```
